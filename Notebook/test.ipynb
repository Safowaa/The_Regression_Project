{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow and Keras imports successful.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, Dense\n",
    "    print(\"TensorFlow and Keras imports successful.\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing TensorFlow/Keras: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A combination of all the functions used in this notebook for creating a pipeline\n",
    "def combined_preprocessing(df, date_column='date', impute_column='dcoilwtico', impute_strategy='mean', columns_to_standardize=None):\n",
    "    # Convert date column to datetime\n",
    "    df[date_column] = pd.to_datetime(df[date_column])\n",
    "\n",
    "    # Add date parts\n",
    "    df['day'] = df[date_column].dt.day_name()\n",
    "    df['month'] = df[date_column].dt.month\n",
    "    df['week'] = df[date_column].dt.isocalendar().week.astype(int)\n",
    "    df['year'] = df[date_column].dt.year\n",
    "\n",
    "    # Analyze missing values and duplicates\n",
    "    print(\"Proportion of missing values:\")\n",
    "    print((df.isnull().mean() * 100).round(2))\n",
    "    print(\"\\nNumber of duplicated rows:\", df.duplicated().sum())\n",
    "\n",
    "    # Impute missing values\n",
    "    if impute_column in df.columns:\n",
    "        imputer = SimpleImputer(strategy=impute_strategy)\n",
    "        df[impute_column] = imputer.fit_transform(df[[impute_column]])\n",
    "    else:\n",
    "        print(f\"Column '{impute_column}' not found in DataFrame. Skipping imputation step.\")\n",
    "\n",
    "    # Standardize certain columns\n",
    "    if columns_to_standardize is not None:\n",
    "        for column in columns_to_standardize:\n",
    "            if column in df.columns:\n",
    "                df[column] = df[column].apply(lambda x: 'Yes' if x is True else ('No' if x is False else ('Not Applicable' if pd.isna(x) else x)))\n",
    "            else:\n",
    "                print(f\"Column '{column}' not found in DataFrame. Skipping standardization step for this column.\")\n",
    "    \n",
    "     # Print value counts for standardized columns\n",
    "    if columns_to_standardize is not None:\n",
    "        for column in columns_to_standardize:\n",
    "            if column in df.columns:\n",
    "                print(df[column].value_counts())\n",
    "                print()\n",
    "\n",
    "    # Convert numeric columns to optimize memory usage\n",
    "    float64_cols = df.select_dtypes(include=['float64'])\n",
    "    df[float64_cols.columns] = float64_cols.astype('float32')\n",
    "    float_cols = df.select_dtypes(include=['float'])\n",
    "    df[float_cols.columns] = float_cols.apply(pd.to_numeric, downcast='float')\n",
    "    int_cols = df.select_dtypes(include=['int'])\n",
    "    df[int_cols.columns] = int_cols.apply(pd.to_numeric, downcast='integer')\n",
    "\n",
    "    return df\n",
    "\n",
    "# Define columns to standardize\n",
    "columns_to_standardize = ['holiday_type', 'locale', 'locale_name', 'description', 'transferred']\n",
    "\n",
    "# # Apply combined preprocessing function\n",
    "# df_test_processed = combined_preprocessing(df_test, date_column='date', impute_column='dcoilwtico', columns_to_standardize=columns_to_standardize)\n",
    "# print(df_test_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the preprocessing pipeline\n",
    "preprocessing_pipeline = Pipeline(steps=[\n",
    "    ('combined_preprocessing', FunctionTransformer(combined_preprocessing, \n",
    "                                                   kw_args={\n",
    "                                                       'date_column': 'date', \n",
    "                                                       'impute_column': 'dcoilwtico', \n",
    "                                                       'columns_to_standardize': columns_to_standardize\n",
    "                                                   }))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>family</th>\n",
       "      <th>onpromotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3000888</td>\n",
       "      <td>2017-08-16</td>\n",
       "      <td>1</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3000889</td>\n",
       "      <td>2017-08-16</td>\n",
       "      <td>1</td>\n",
       "      <td>BABY CARE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3000890</td>\n",
       "      <td>2017-08-16</td>\n",
       "      <td>1</td>\n",
       "      <td>BEAUTY</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3000891</td>\n",
       "      <td>2017-08-16</td>\n",
       "      <td>1</td>\n",
       "      <td>BEVERAGES</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3000892</td>\n",
       "      <td>2017-08-16</td>\n",
       "      <td>1</td>\n",
       "      <td>BOOKS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id        date  store_nbr      family  onpromotion\n",
       "0  3000888  2017-08-16          1  AUTOMOTIVE            0\n",
       "1  3000889  2017-08-16          1   BABY CARE            0\n",
       "2  3000890  2017-08-16          1      BEAUTY            2\n",
       "3  3000891  2017-08-16          1   BEVERAGES           20\n",
       "4  3000892  2017-08-16          1       BOOKS            0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  The data has been split already, the test data is different from the train data\n",
    "# Read the first test data into a dataframe\n",
    "test_1 = pd.read_csv(r\"C:\\Users\\Safowaa\\Documents\\Azibiafrica\\AzubiPython\\The_Regression_Project\\Project_data\\test.csv\")\n",
    "test_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3000888</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3000889</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3000890</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3000891</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3000892</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  sales\n",
       "0  3000888    0.0\n",
       "1  3000889    0.0\n",
       "2  3000890    0.0\n",
       "3  3000891    0.0\n",
       "4  3000892    0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read the second test data into a dataframe\n",
    "test_2 = pd.read_csv(r\"C:\\Users\\Safowaa\\Documents\\Azibiafrica\\AzubiPython\\The_Regression_Project\\Project_data\\sample_submission_test.csv\")\n",
    "test_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28512 entries, 0 to 28511\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   id           28512 non-null  int64  \n",
      " 1   date         28512 non-null  object \n",
      " 2   store_nbr    28512 non-null  int64  \n",
      " 3   family       28512 non-null  object \n",
      " 4   onpromotion  28512 non-null  int64  \n",
      " 5   sales        28512 non-null  float64\n",
      "dtypes: float64(1), int64(3), object(2)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# combine all the test data into one for testing\n",
    "df_test = pd.merge(test_1, test_2 , on='id', how='left')\n",
    "\n",
    "# Display a info of the dataframe\n",
    "df_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "` Tranforming the test data set `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of missing values:\n",
      "id             0.0\n",
      "date           0.0\n",
      "store_nbr      0.0\n",
      "family         0.0\n",
      "onpromotion    0.0\n",
      "sales          0.0\n",
      "day            0.0\n",
      "month          0.0\n",
      "week           0.0\n",
      "year           0.0\n",
      "dtype: float64\n",
      "\n",
      "Number of duplicated rows: 0\n",
      "Column 'dcoilwtico' not found in DataFrame. Skipping imputation step.\n",
      "Column 'holiday_type' not found in DataFrame. Skipping standardization step for this column.\n",
      "Column 'locale' not found in DataFrame. Skipping standardization step for this column.\n",
      "Column 'locale_name' not found in DataFrame. Skipping standardization step for this column.\n",
      "Column 'description' not found in DataFrame. Skipping standardization step for this column.\n",
      "Column 'transferred' not found in DataFrame. Skipping standardization step for this column.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>family</th>\n",
       "      <th>onpromotion</th>\n",
       "      <th>sales</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>week</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3000888</td>\n",
       "      <td>2017-08-16</td>\n",
       "      <td>1</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>8</td>\n",
       "      <td>33</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3000889</td>\n",
       "      <td>2017-08-16</td>\n",
       "      <td>1</td>\n",
       "      <td>BABY CARE</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>8</td>\n",
       "      <td>33</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3000890</td>\n",
       "      <td>2017-08-16</td>\n",
       "      <td>1</td>\n",
       "      <td>BEAUTY</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>8</td>\n",
       "      <td>33</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3000891</td>\n",
       "      <td>2017-08-16</td>\n",
       "      <td>1</td>\n",
       "      <td>BEVERAGES</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>8</td>\n",
       "      <td>33</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3000892</td>\n",
       "      <td>2017-08-16</td>\n",
       "      <td>1</td>\n",
       "      <td>BOOKS</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>8</td>\n",
       "      <td>33</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28507</th>\n",
       "      <td>3029395</td>\n",
       "      <td>2017-08-31</td>\n",
       "      <td>9</td>\n",
       "      <td>POULTRY</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>8</td>\n",
       "      <td>35</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28508</th>\n",
       "      <td>3029396</td>\n",
       "      <td>2017-08-31</td>\n",
       "      <td>9</td>\n",
       "      <td>PREPARED FOODS</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>8</td>\n",
       "      <td>35</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28509</th>\n",
       "      <td>3029397</td>\n",
       "      <td>2017-08-31</td>\n",
       "      <td>9</td>\n",
       "      <td>PRODUCE</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>8</td>\n",
       "      <td>35</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28510</th>\n",
       "      <td>3029398</td>\n",
       "      <td>2017-08-31</td>\n",
       "      <td>9</td>\n",
       "      <td>SCHOOL AND OFFICE SUPPLIES</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>8</td>\n",
       "      <td>35</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28511</th>\n",
       "      <td>3029399</td>\n",
       "      <td>2017-08-31</td>\n",
       "      <td>9</td>\n",
       "      <td>SEAFOOD</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>8</td>\n",
       "      <td>35</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28512 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id       date  store_nbr                      family  onpromotion  \\\n",
       "0      3000888 2017-08-16          1                  AUTOMOTIVE            0   \n",
       "1      3000889 2017-08-16          1                   BABY CARE            0   \n",
       "2      3000890 2017-08-16          1                      BEAUTY            2   \n",
       "3      3000891 2017-08-16          1                   BEVERAGES           20   \n",
       "4      3000892 2017-08-16          1                       BOOKS            0   \n",
       "...        ...        ...        ...                         ...          ...   \n",
       "28507  3029395 2017-08-31          9                     POULTRY            1   \n",
       "28508  3029396 2017-08-31          9              PREPARED FOODS            0   \n",
       "28509  3029397 2017-08-31          9                     PRODUCE            1   \n",
       "28510  3029398 2017-08-31          9  SCHOOL AND OFFICE SUPPLIES            9   \n",
       "28511  3029399 2017-08-31          9                     SEAFOOD            0   \n",
       "\n",
       "       sales        day  month  week  year  \n",
       "0        0.0  Wednesday      8    33  2017  \n",
       "1        0.0  Wednesday      8    33  2017  \n",
       "2        0.0  Wednesday      8    33  2017  \n",
       "3        0.0  Wednesday      8    33  2017  \n",
       "4        0.0  Wednesday      8    33  2017  \n",
       "...      ...        ...    ...   ...   ...  \n",
       "28507    0.0   Thursday      8    35  2017  \n",
       "28508    0.0   Thursday      8    35  2017  \n",
       "28509    0.0   Thursday      8    35  2017  \n",
       "28510    0.0   Thursday      8    35  2017  \n",
       "28511    0.0   Thursday      8    35  2017  \n",
       "\n",
       "[28512 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply the pipeline to the test data\n",
    "preprocessing_pipeline.fit_transform(df_test)\n",
    "df_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Dataset Splitting`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for machine learning models\n",
    "train_ml, val_ml = train_test_split(df_train, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the dataframe \n",
    "df_train_copy1 = df_train.copy()   #RandomForest\n",
    "df_train_copy2 = df_train.copy()   #XGBoost\n",
    "df_test_copy1 = df_test.copy()     #RandonForest\n",
    "df_test_copy2 = df_test.copy()     #XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical and numerical columns\n",
    "categorical_columns = ['City', 'family', 'day', 'state', 'store_type ','holiday_type', 'locale', 'locale_name', 'description', 'transferred' ]  \n",
    "\n",
    "numerical_columns = df_train_copy1.columns.difference(categorical_columns + ['sales'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical and numerical columns\n",
    "categorical_columns = ['family', 'day']\n",
    "numerical_columns = df_test_copy1.columns.difference(categorical_columns + ['sales'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for RandomForest and XGBoost\n",
    "train_ml1, val_ml1 = train_test_split(df_train_copy1, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_columns),\n",
    "        ('cat', OneHotEncoder(), categorical_columns)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data for machine learning model RandomForest\n",
    "\n",
    "Xdf_train_copy1 = df_train_copy1.drop(\"sales\", axis= 1)\n",
    "\n",
    "ydf_train_copy1 = df_train_copy1[\"sales\"]\n",
    "\n",
    "Xdf_test_copy1 = df_test_copy1.drop(\"sales\", axis= 1)\n",
    "\n",
    "ydf_test_copy1 = df_test_copy1[\"sales\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Features Creation` **&** `Encoding `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For Time Series Models (ARIMA, SARIMA, ETS, Prophet):\n",
    "- Time-based Features:\n",
    "- Lag Features: Create features that represent past values of the target variable (e.g., sales) at different time lags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['lag_1'] = df_train['sales'].shift(1)\n",
    "df_train['lag_2'] = df_train['sales'].shift(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lag features are created by shifting the target variable  (sales) by a certain number of time periods (lags). This means you use past values of the variable to predict future values.\n",
    "- y including past values, the model can learn patterns such as trends and seasonality, improving its predictive accuracy.\n",
    "- Lag features allow the model to understand how past values of the series affect the current value. This is important in time series data where previous observations can have a significant impact on future outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rolling Statistics: Calculate rolling means, sums, or standard deviations over a window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['rolling_mean_7'] = df_train['sales'].rolling(window=7).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                 NaN\n",
       "1                 NaN\n",
       "2                 NaN\n",
       "3                 NaN\n",
       "4                 NaN\n",
       "              ...    \n",
       "2805226    208.194427\n",
       "2805227    226.701998\n",
       "2805228    570.806141\n",
       "2805229    523.916428\n",
       "2805230    451.630713\n",
       "Name: rolling_mean_7, Length: 2805231, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train['rolling_mean_7']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Our  7-day rolling mean calculates the average of the current day and the previous six days.\n",
    "- It smooths out short-term fluctuations and highlights longer-term trends or cycles in the data.\n",
    "- It helps in identifying trends over time, making it easier to see patterns or changes in direction.\n",
    "- Finally, helps to visualize trends in the data over a weekly period, reducing day-to-day variability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Seasonality and Trends:\n",
    "- Seasonal Indicators: For SARIMA and ETS, use indicators for seasons or holidays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['is_holiday'] = df_train['holiday_type'].apply(lambda x: 1 if x == 'Holiday' else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All dates are valid.\n",
      "All dates are valid.\n"
     ]
    }
   ],
   "source": [
    "# Check for invalid dates in the date column\n",
    "def check_invalid_dates(df, date_column='date'):\n",
    "    try:\n",
    "        pd.to_datetime(df[date_column])\n",
    "        print(\"All dates are valid.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Invalid date found: {e}\")\n",
    "        \n",
    "check_invalid_dates(df_train)\n",
    "check_invalid_dates(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Identify and remove rows with invalid dates\n",
    "def clean_invalid_dates(df, date_column='date'):\n",
    "    df = df.reset_index()  # Reset index to ensure 'date' is a column\n",
    "    df[date_column] = pd.to_datetime(df[date_column], errors='coerce')\n",
    "    invalid_dates = df[df[date_column].isna()]\n",
    "    if not invalid_dates.empty:\n",
    "        print(\"Removing rows with invalid dates:\")\n",
    "        print(invalid_dates)\n",
    "        df = df.dropna(subset=[date_column])\n",
    "    return df.set_index(date_column)  # Set 'date' column back as index\n",
    "\n",
    "df_train = clean_invalid_dates(df_train)\n",
    "df_test = clean_invalid_dates(df_test)\n",
    "\n",
    "# Ensure the date columns are in the correct format for Prophet\n",
    "df_train['ds'] = df_train.index\n",
    "df_train['y'] = df_train['sales']\n",
    "df_test['ds'] = df_test.index\n",
    "df_test['y'] = df_test['sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_columns(df_train, df_test):\n",
    "    # Get columns in df_train that are not in df_test\n",
    "    missing_cols = set(df_train.columns) - set(df_test.columns)\n",
    "    \n",
    "    # Add missing columns to df_test and fill them with zeros\n",
    "    for col in missing_cols:\n",
    "        df_test[col] = 0\n",
    "    \n",
    "    return df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = align_columns(df_train, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For Machine Learning Models (RandomForest, XGBoost):\n",
    "- Lag Features and Rolling Statistics: As with time series models, these are useful for capturing temporal patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Product of Features: Create new features by combining existing features to capture interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_copy1['sales_onpromotion'] = df_train_copy1['sales'] * df_train_copy1['onpromotion']\n",
    "df_test_copy1['sales_onpromotion'] = df_test_copy1['sales'] * df_test_copy1['onpromotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add missing columns in the test dataset and fill them with zeros\n",
    "missing_cols = set(df_train_copy1.columns) - set(df_test_copy1.columns)\n",
    "for col in missing_cols:\n",
    "    df_test_copy1[col] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure consistent columns in train and test sets\n",
    "df_train_copy1, df_test_copy1 = df_train_copy1.align(df_test, join='inner', axis=1, fill_value=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of categorical columns to be encoded\n",
    "categorical_columns = ['family', 'city', 'day', 'state', 'store_type', 'holiday_type', 'locale', 'locale_name', 'description', 'transferred']\n",
    "\n",
    "# Function to create a mapping dictionary for each categorical column\n",
    "def create_mapping(df_train, df_test, column):\n",
    "    unique_values = pd.concat([df_train[column], df_test[column]]).unique()\n",
    "    return {value: idx for idx, value in enumerate(unique_values)}\n",
    "\n",
    "# Create a dictionary to hold mappings for all categorical columns\n",
    "mappings = {col: create_mapping(df_train_copy1, df_test_copy1, col) for col in categorical_columns}\n",
    "\n",
    "# Apply the mappings to both df_train and df_test\n",
    "for col in categorical_columns:\n",
    "    df_train_copy1[col] = df_train_copy1[col].map(mappings[col])\n",
    "    df_test_copy1[col] = df_test_copy1[col].map(mappings[col])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Features Scaling`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Feature Scaling: Normalize or standardize numerical features to help improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical features using OneHotEncoder\n",
    "encoder = OneHotEncoder(drop='first', handle_unknown='ignore')\n",
    "\n",
    "# Ensure numerical_columns do not include datetime columns and are present in both DataFrames\n",
    "numerical_columns_train = Xdf_train_copy1.select_dtypes(include=['number']).columns\n",
    "numerical_columns_test = Xdf_test_copy1.select_dtypes(include=['number']).columns\n",
    "# Find common numerical columns\n",
    "common_numerical_columns = list(set(numerical_columns_train) & set(numerical_columns_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xdf_train_copy1 = Xdf_train_copy1.drop(columns=['cluster', 'transactions', 'dcoilwtico'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id       date  store_nbr      family  onpromotion      day     month  \\\n",
      "0 -1.791702 2013-01-01  -0.124337  AUTOMOTIVE    -0.223197  Tuesday -1.562782   \n",
      "1 -1.791701 2013-01-01  -0.124337   BABY CARE    -0.223197  Tuesday -1.562782   \n",
      "2 -1.791700 2013-01-01  -0.124337      BEAUTY    -0.223197  Tuesday -1.562782   \n",
      "3 -1.791699 2013-01-01  -0.124337   BEVERAGES    -0.223197  Tuesday -1.562782   \n",
      "4 -1.791698 2013-01-01  -0.124337       BOOKS    -0.223197  Tuesday -1.562782   \n",
      "\n",
      "       week      year     city        state store_type holiday_type    locale  \\\n",
      "0 -1.658371 -1.419608  Salinas  Santa Elena          D      Holiday  National   \n",
      "1 -1.658371 -1.419608  Salinas  Santa Elena          D      Holiday  National   \n",
      "2 -1.658371 -1.419608  Salinas  Santa Elena          D      Holiday  National   \n",
      "3 -1.658371 -1.419608  Salinas  Santa Elena          D      Holiday  National   \n",
      "4 -1.658371 -1.419608  Salinas  Santa Elena          D      Holiday  National   \n",
      "\n",
      "  locale_name         description transferred  \n",
      "0     Ecuador  Primer dia del ano          No  \n",
      "1     Ecuador  Primer dia del ano          No  \n",
      "2     Ecuador  Primer dia del ano          No  \n",
      "3     Ecuador  Primer dia del ano          No  \n",
      "4     Ecuador  Primer dia del ano          No  \n",
      "         id       date  store_nbr      family  onpromotion        day  \\\n",
      "0  1.675318 2017-08-16  -1.662039  AUTOMOTIVE    -0.223197  Wednesday   \n",
      "1  1.675319 2017-08-16  -1.662039   BABY CARE    -0.223197  Wednesday   \n",
      "2  1.675320 2017-08-16  -1.662039      BEAUTY    -0.066459  Wednesday   \n",
      "3  1.675321 2017-08-16  -1.662039   BEVERAGES     1.344180  Wednesday   \n",
      "4  1.675322 2017-08-16  -1.662039       BOOKS    -0.223197  Wednesday   \n",
      "\n",
      "      month      week      year  \n",
      "0  0.518878  0.520609  1.550692  \n",
      "1  0.518878  0.520609  1.550692  \n",
      "2  0.518878  0.520609  1.550692  \n",
      "3  0.518878  0.520609  1.550692  \n",
      "4  0.518878  0.520609  1.550692  \n"
     ]
    }
   ],
   "source": [
    "# Feature Scaling for numerical features\n",
    "scaler = StandardScaler()\n",
    "Xdf_train_copy1[common_numerical_columns] = scaler.fit_transform(Xdf_train_copy1[common_numerical_columns])\n",
    "Xdf_test_copy1[common_numerical_columns] = scaler.transform(Xdf_test_copy1[common_numerical_columns])\n",
    "\n",
    "# Print the scaled DataFrames to verify\n",
    "print(Xdf_train_copy1.head())\n",
    "print(Xdf_test_copy1.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporarily reset the index to align the data\n",
    "Xdf_train_copy1_reset = Xdf_train_copy1.reset_index()\n",
    "Xdf_test_copy1_reset = Xdf_test_copy1.reset_index()\n",
    "df_train_copy1_reset = df_train_copy1.reset_index()\n",
    "df_test_copy1_reset = df_test_copy1.reset_index()\n",
    "\n",
    "# Transfer 'sales' column to Xdf_train_copy1 and Xdf_test_copy1\n",
    "Xdf_train_copy1_reset['sales'] = df_train_copy1_reset['sales']\n",
    "Xdf_test_copy1_reset['sales'] = df_test_copy1_reset['sales']\n",
    "\n",
    "# Restore the original date index\n",
    "Xdf_train_copy1 = Xdf_train_copy1_reset.set_index('date')\n",
    "Xdf_test_copy1 = Xdf_test_copy1_reset.set_index('date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Categorical Encoding:\n",
    "\n",
    "- One-Hot Encoding: For categorical variables such as family, city, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical features using OneHotEncoder\n",
    "encoder = OneHotEncoder(drop='first', handle_unknown='ignore')\n",
    "\n",
    "Xdf_train_copy1 = encoder.fit_transform(Xdf_train_copy1)\n",
    "Xdf_test_copy1 = encoder.fit_transform(Xdf_test_copy1)\n",
    "\n",
    "# Align the train and test sets\n",
    "# Xdf_train_copy1_encoded, Xdf_test_copy1_encoded = Xdf_train_copy1_encoded.align(Xdf_test_copy1_encoded, join='left', axis=1, fill_value=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Modeling \n",
    "### Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "\n",
    "models['ARIMA'] = lambda df: ARIMA(df['sales'], order=(5,1,0)).fit()\n",
    "models['SARIMA'] = lambda df: SARIMAX(df['sales'], order=(1, 1, 1), seasonal_order=(1, 1, 1, 12)).fit()\n",
    "models['ETS'] = lambda df: ExponentialSmoothing(df['sales'], seasonal='add', seasonal_periods=12).fit()\n",
    "models['Prophet'] = lambda df: Prophet().fit(pd.DataFrame({'ds': df.index, 'y': df['sales']}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_train_predict(model, X_train, y_train, X_test, batch_size=10000):\n",
    "    # Shuffle the training data\n",
    "    X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "    # Create empty lists to hold predictions\n",
    "    val_predictions = []\n",
    "    test_predictions = []\n",
    "\n",
    "    # Determine the number of batches\n",
    "    num_batches = int(np.ceil(X_train.shape[0] / batch_size))\n",
    "\n",
    "    # Train in batches\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, X_train.shape[0])\n",
    "        \n",
    "        X_batch = X_train[start_idx:end_idx]\n",
    "        y_batch = y_train[start_idx:end_idx]\n",
    "        \n",
    "        # Fit the model on the batch\n",
    "        model.fit(X_batch, y_batch)\n",
    "\n",
    "        # Predict on validation and test data\n",
    "        val_pred_batch = model.predict(X_test)\n",
    "        test_pred_batch = model.predict(X_test)\n",
    "\n",
    "        val_predictions.extend(val_pred_batch)\n",
    "        test_predictions.extend(test_pred_batch)\n",
    "\n",
    "    return np.array(val_predictions), np.array(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict(model_name, df_train, val_ml, df_test):\n",
    "    # Ensure the model name exists in the dictionary\n",
    "    if model_name not in models:\n",
    "        raise ValueError(f\"Model '{model_name}' is not defined in the models dictionary.\")\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = models[model_name](df_train)\n",
    "\n",
    "    # Check model type and make predictions\n",
    "    if model_name in ['ARIMA', 'SARIMA', 'ETS']:\n",
    "        val_predictions = model.forecast(len(val_ml))\n",
    "        test_predictions = model.forecast(len(df_test))\n",
    "    elif model_name == 'Prophet':\n",
    "        future_val = pd.DataFrame({'ds': val_ml.index})\n",
    "        future_test = pd.DataFrame({'ds': df_test.index})\n",
    "        val_predictions = model.predict(future_val)['yhat']\n",
    "        test_predictions = model.predict(future_test)['yhat']\n",
    "    else:  # For machine learning models\n",
    "        val_predictions = model.predict(val_ml.drop('sales', axis=1))\n",
    "        test_predictions = model.predict(df_test.drop('sales', axis=1))\n",
    "    \n",
    "    return val_predictions, test_predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model **1)**   ARIMA\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'ARIMA' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model on Validation and testing data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predictions, test_predictions = train_and_predict(model_name, df_train, val_ml, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Predictions: 2805231    394.162823\n",
      "2805232    745.195494\n",
      "2805233    787.659829\n",
      "2805234    722.388079\n",
      "2805235    354.675957\n",
      "              ...    \n",
      "3366273    576.338300\n",
      "3366274    576.338300\n",
      "3366275    576.338300\n",
      "3366276    576.338300\n",
      "3366277    576.338300\n",
      "Name: predicted_mean, Length: 561047, dtype: float64\n",
      "Test Predictions: 2805231    394.162823\n",
      "2805232    745.195494\n",
      "2805233    787.659829\n",
      "2805234    722.388079\n",
      "2805235    354.675957\n",
      "              ...    \n",
      "2833738    576.338300\n",
      "2833739    576.338300\n",
      "2833740    576.338300\n",
      "2833741    576.338300\n",
      "2833742    576.338300\n",
      "Name: predicted_mean, Length: 28512, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation Predictions:\", val_predictions)\n",
    "print(\"Test Predictions:\", test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model **3)** ETS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'ETS'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model on Validation and testing data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predictions, test_predictions = train_and_predict(model_name, df_train, val_ml, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Predictions: 2805231    1092.234801\n",
      "2805232     220.136296\n",
      "2805233     172.646969\n",
      "2805234    1294.874635\n",
      "2805235     236.930000\n",
      "              ...     \n",
      "3366273    1099.836149\n",
      "3366274     228.045084\n",
      "3366275     181.546279\n",
      "3366276    1019.008406\n",
      "3366277     235.275733\n",
      "Length: 561047, dtype: float64\n",
      "Test Predictions: 2805231    1092.234801\n",
      "2805232     220.136296\n",
      "2805233     172.646969\n",
      "2805234    1294.874635\n",
      "2805235     236.930000\n",
      "              ...     \n",
      "2833738     228.045084\n",
      "2833739     181.546279\n",
      "2833740    1019.008406\n",
      "2833741     235.275733\n",
      "2833742     163.793417\n",
      "Length: 28512, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation Predictions:\", val_predictions)\n",
    "print(\"Test Predictions:\", test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model **4)** Prophet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name ='Prophet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model on Validation and testing data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predictions, test_predictions = train_and_predict(model_name, df_train, val_ml, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Predictions: 0         259.430920\n",
      "1         259.430920\n",
      "2         259.430920\n",
      "3         259.430920\n",
      "4         259.430920\n",
      "             ...    \n",
      "561042    415.830814\n",
      "561043    415.830814\n",
      "561044    415.830814\n",
      "561045    415.830814\n",
      "561046    415.830814\n",
      "Name: yhat, Length: 561047, dtype: float64\n",
      "Test Predictions: 0        431.278407\n",
      "1        431.278407\n",
      "2        431.278407\n",
      "3        431.278407\n",
      "4        431.278407\n",
      "            ...    \n",
      "28507    421.910667\n",
      "28508    421.910667\n",
      "28509    421.910667\n",
      "28510    421.910667\n",
      "28511    421.910667\n",
      "Name: yhat, Length: 28512, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation Predictions:\", val_predictions)\n",
    "print(\"Test Predictions:\", test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model **5)** RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_train_predict(model, Xdf_train_copy1, val_ml1, Xdf_test_copy1, batch_size=10000):\n",
    "    # Shuffle the training data\n",
    "    Xdf_train_copy1 = shuffle(Xdf_train_copy1)\n",
    "\n",
    "    # Create empty lists to hold predictions\n",
    "    val_predictions = []\n",
    "    test_predictions = []\n",
    "\n",
    "    # Determine the number of batches\n",
    "    num_batches = int(np.ceil(Xdf_train_copy1.shape[0] / batch_size))\n",
    "\n",
    "    # Train in batches\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, Xdf_train_copy1.shape[0])\n",
    "        \n",
    "        batch = Xdf_train_copy1.iloc[start_idx:end_idx]\n",
    "        \n",
    "        # Ensure the batch has no datetime columns\n",
    "        batch = batch.select_dtypes(exclude=['datetime64'])\n",
    "        \n",
    "        # Fit the model on the batch\n",
    "        model.fit(batch, batch['sales'])\n",
    "\n",
    "        # Predict on validation and test data\n",
    "        val_pred_batch = model.predict(val_ml1.drop('sales', axis=1))\n",
    "        test_pred_batch = model.predict(Xdf_test_copy1.drop('sales', axis=1))\n",
    "\n",
    "        val_predictions.extend(val_pred_batch)\n",
    "        test_predictions.extend(test_pred_batch)\n",
    "\n",
    "    return np.array(val_predictions), np.array(test_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define models with batch processing\n",
    "models = {}\n",
    "\n",
    "models['RandomForest'] = lambda df: batch_train_predict(RandomForestRegressor(n_estimators=100), Xdf_train_copy1, val_ml1, Xdf_test_copy1, batch_size=10000)\n",
    "models['XGBoost'] = lambda df: batch_train_predict(xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100), df, val_ml1, Xdf_test_copy1, batch_size=10000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train and evaluate models\n",
    "model_name = 'RandomForest'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model on Validation and testing data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'EGGS'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20244\\4238110505.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m----> 1\u001b[1;33m \u001b[0mval_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_and_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_train_copy1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ml1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_test_copy1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20244\\1966890807.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(model_name, df_train, val_ml, df_test)\u001b[0m\n",
      "\u001b[0;32m      3\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m      4\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Model '{model_name}' is not defined in the models dictionary.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m      6\u001b[0m     \u001b[1;31m# Initialize the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m      9\u001b[0m     \u001b[1;31m# Check model type and make predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m     10\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'ARIMA'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'SARIMA'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ETS'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20244\\346547160.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(df)\u001b[0m\n",
      "\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'RandomForest'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_train_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRandomForestRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXdf_train_copy1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ml1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXdf_test_copy1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20244\\1661096646.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(model, Xdf_train_copy1, val_ml1, Xdf_test_copy1, batch_size)\u001b[0m\n",
      "\u001b[0;32m     19\u001b[0m         \u001b[1;31m# Ensure the batch has no datetime columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m     20\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_dtypes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'datetime64'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m     22\u001b[0m         \u001b[1;31m# Fit the model on the batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sales'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m     25\u001b[0m         \u001b[1;31m# Predict on validation and test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m     26\u001b[0m         \u001b[0mval_pred_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_ml1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'sales'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;32mc:\\Users\\Safowaa\\Documents\\Azibiafrica\\AzubiPython\\The_Regression_Project\\virtual\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1469\u001b[0m                 skip_parameter_validation=(\n",
      "\u001b[0;32m   1470\u001b[0m                     \u001b[0mprefer_skip_nested_validation\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m   1471\u001b[0m                 )\n",
      "\u001b[0;32m   1472\u001b[0m             ):\n",
      "\u001b[1;32m-> 1473\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Safowaa\\Documents\\Azibiafrica\\AzubiPython\\The_Regression_Project\\virtual\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n",
      "\u001b[0;32m    359\u001b[0m         \u001b[1;31m# Validate or convert input data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    360\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    361\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sparse multilabel-indicator for y is not supported.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m--> 363\u001b[1;33m         X, y = self._validate_data(\n",
      "\u001b[0m\u001b[0;32m    364\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    365\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    366\u001b[0m             \u001b[0mmulti_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;32mc:\\Users\\Safowaa\\Documents\\Azibiafrica\\AzubiPython\\The_Regression_Project\\virtual\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n",
      "\u001b[0;32m    646\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;34m\"estimator\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcheck_y_params\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    647\u001b[0m                     \u001b[0mcheck_y_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mdefault_check_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    648\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"y\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    649\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m--> 650\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    651\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    652\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    653\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ensure_2d\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;32mc:\\Users\\Safowaa\\Documents\\Azibiafrica\\AzubiPython\\The_Regression_Project\\virtual\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n",
      "\u001b[0;32m   1297\u001b[0m         raise ValueError(\n",
      "\u001b[0;32m   1298\u001b[0m             \u001b[1;34mf\"{estimator_name} requires y to be passed, but the target y is None\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m   1299\u001b[0m         )\n",
      "\u001b[0;32m   1300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m-> 1301\u001b[1;33m     X = check_array(\n",
      "\u001b[0m\u001b[0;32m   1302\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m   1303\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m   1304\u001b[0m         \u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;32mc:\\Users\\Safowaa\\Documents\\Azibiafrica\\AzubiPython\\The_Regression_Project\\virtual\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n",
      "\u001b[0;32m   1009\u001b[0m                         )\n",
      "\u001b[0;32m   1010\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m   1011\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m   1012\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m-> 1013\u001b[1;33m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   1014\u001b[0m                 raise ValueError(\n",
      "\u001b[0;32m   1015\u001b[0m                     \u001b[1;34m\"Complex data not supported\\n{}\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m   1016\u001b[0m                 ) from complex_warning\n",
      "\n",
      "\u001b[1;32mc:\\Users\\Safowaa\\Documents\\Azibiafrica\\AzubiPython\\The_Regression_Project\\virtual\\lib\\site-packages\\sklearn\\utils\\_array_api.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(array, dtype, order, copy, xp, device)\u001b[0m\n",
      "\u001b[0;32m    747\u001b[0m         \u001b[1;31m# Use NumPy API to support order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    748\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    749\u001b[0m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    750\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m--> 751\u001b[1;33m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    752\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    753\u001b[0m         \u001b[1;31m# At this point array is a NumPy ndarray. We convert it to an array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    754\u001b[0m         \u001b[1;31m# container that is consistent with the input's namespace.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;32mc:\\Users\\Safowaa\\Documents\\Azibiafrica\\AzubiPython\\The_Regression_Project\\virtual\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, dtype, copy)\u001b[0m\n",
      "\u001b[0;32m   2149\u001b[0m     def __array__(\n",
      "\u001b[0;32m   2150\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDTypeLike\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool_t\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m   2151\u001b[0m     ) -> np.ndarray:\n",
      "\u001b[0;32m   2152\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m-> 2153\u001b[1;33m         \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   2154\u001b[0m         if (\n",
      "\u001b[0;32m   2155\u001b[0m             \u001b[0mastype_is_view\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m   2156\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0musing_copy_on_write\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'EGGS'"
     ]
    }
   ],
   "source": [
    "val_predictions, test_predictions = train_and_predict(model_name, df_train_copy1, val_ml1, df_test_copy1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Validation Predictions:\", val_predictions)\n",
    "print(\"Test Predictions:\", test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model **6)** XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name ='XGBoost'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model on Validation and testing data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predictions, test_predictions = train_and_predict(model_name, df_train_copy1, val_ml1, df_test_copy1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Validation Predictions:\", val_predictions)\n",
    "print(\"Test Predictions:\", test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return mae, mse, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for model_name in models.keys():\n",
    "    try:\n",
    "        val_predictions, test_predictions = train_and_predict(model_name, df_train, val_ml, df_test)\n",
    "        mae, mse, rmse = evaluate_model(val_ml['sales'], val_predictions)\n",
    "        results.append({\n",
    "            'Model_Name': model_name,\n",
    "            'MAE': mae,\n",
    "            'MSE': mse,\n",
    "            'RMSE': rmse,\n",
    "            'Details': 'Validation Set'\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error with model {model_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results1 = []\n",
    "\n",
    "for model_name in models.keys():\n",
    "    try:\n",
    "        val_predictions, test_predictions = batch_train_predict(model_name, df_train_copy1, val_ml1, df_test_copy1)\n",
    "        mae, mse, rmse = evaluate_model(val_ml1['sales'], val_predictions)\n",
    "        results.append({\n",
    "            'Model_Name': model_name,\n",
    "            'MAE': mae,\n",
    "            'MSE': mse,\n",
    "            'RMSE': rmse,\n",
    "            'Details': 'Validation Set'\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error with model {model_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame\n",
    "results_df1 = pd.DataFrame(results1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by MAE (or any other metric)\n",
    "results_df = results_df.sort_values(by='MAE')\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a pandas dataframe that will allow you to compare your models.\n",
    "\n",
    "\n",
    "|     | Model_Name     | Metric (metric_name)    | Details  |\n",
    "|:---:|:--------------:|:--------------:|:-----------------:|\n",
    "| 0   |  -             |  -             | -                 |\n",
    "| 1   |  -             |  -             | -                 |\n",
    "\n",
    "\n",
    "You might use the pandas dataframe method `.sort_values()` to sort the dataframe regarding the metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters tuning \n",
    "\n",
    "Fine-tune the Top-k models (3 < k < 5) using a ` GridSearchCV`  (that is in sklearn.model_selection\n",
    ") to find the best hyperparameters and achieve the maximum performance of each of the Top-k models, then compare them again to select the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export key components\n",
    "Here is the section to **export** the important ML objects that will be use to develop an app: *Encoder, Scaler, ColumnTransformer, Model, Pipeline, etc*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
